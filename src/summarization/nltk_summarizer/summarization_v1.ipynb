{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266369\n",
      "14334\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Reinforcement_learning')\n",
    "article = scraped_data.read()\n",
    "\n",
    "print(len(article))\n",
    "print(len(article.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "parsed_article = bs.BeautifulSoup(article, 'lxml')\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "print(len(paragraphs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20410\n"
     ]
    }
   ],
   "source": [
    "article_text = ''\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text\n",
    "\n",
    "print(len(article_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-edeb7bf2f42d>:2: FutureWarning: Possible nested set at position 1\n",
      "  article_text = re.sub(r'[[0-9]*]', ' ', article_text)\n"
     ]
    }
   ],
   "source": [
    "# Removing Square Brackets and Extra Spaces\n",
    "article_text = re.sub(r'[[0-9]*]', ' ', article_text)\n",
    "article_text = re.sub(r's+', ' ', article_text)\n",
    "\n",
    "print(len(article_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20304\n"
     ]
    }
   ],
   "source": [
    "# Removing special characters and digits\n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "formatted_article_text = re.sub(r's+', ' ', formatted_article_text)\n",
    "\n",
    "print(len(formatted_article_text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentence_list = sent_tokenize(article_text)\n",
    "print(len(sentence_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}\n",
    "\n",
    "for word in word_tokenize(formatted_article_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "\n",
    "print(len(word_frequencies))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "maximum_frequency = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
    "\n",
    "print(maximum_frequency)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('di', 1.0)\n",
      "('play', 0.8902439024390244)\n",
      "('tyle', 0.8780487804878049)\n",
      "('e', 0.6829268292682927)\n",
      "('tate', 0.5)\n",
      "('action', 0.4878048780487805)\n",
      "('policy', 0.4878048780487805)\n",
      "('learning', 0.4634146341463415)\n",
      "('method', 0.4268292682926829)\n",
      "('ed', 0.34146341463414637)\n",
      "('The', 0.32926829268292684)\n",
      "('problem', 0.3170731707317073)\n",
      "('value', 0.3170731707317073)\n",
      "('function', 0.3048780487804878)\n",
      "('u', 0.25609756097560976)\n",
      "('pi', 0.25609756097560976)\n",
      "('Q', 0.25609756097560976)\n",
      "('reinforcement', 0.24390243902439024)\n",
      "('optimal', 0.21951219512195122)\n",
      "('thi', 0.21951219512195122)\n",
      "('return', 0.2073170731707317)\n",
      "('agent', 0.1951219512195122)\n",
      "('ba', 0.1951219512195122)\n",
      "('algorithm', 0.1951219512195122)\n",
      "('In', 0.18292682926829268)\n",
      "('MDP', 0.15853658536585366)\n",
      "('earch', 0.14634146341463414)\n",
      "('reward', 0.13414634146341464)\n",
      "('A', 0.13414634146341464)\n",
      "('timate', 0.12195121951219512)\n",
      "('environment', 0.10975609756097561)\n",
      "('exploration', 0.10975609756097561)\n",
      "('theory', 0.10975609756097561)\n",
      "('tic', 0.10975609756097561)\n",
      "('may', 0.10975609756097561)\n",
      "('et', 0.10975609756097561)\n",
      "('ob', 0.10975609756097561)\n",
      "('ome', 0.10975609756097561)\n",
      "('pair', 0.0975609756097561)\n",
      "('difference', 0.0975609756097561)\n",
      "('large', 0.0975609756097561)\n",
      "('called', 0.0975609756097561)\n",
      "('approximation', 0.0975609756097561)\n",
      "('tep', 0.0975609756097561)\n",
      "('performance', 0.0975609756097561)\n",
      "('con', 0.0975609756097561)\n",
      "('two', 0.0975609756097561)\n",
      "('pace', 0.0975609756097561)\n",
      "('policie', 0.0975609756097561)\n",
      "('V', 0.0975609756097561)\n",
      "('given', 0.0975609756097561)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "sorted_dict = {k: v for k, v in sorted(word_frequencies.items(), key=lambda item_: item_[1], reverse=True)}\n",
    "\n",
    "for item in sorted_dict.items():\n",
    "    print(item)\n",
    "    i += 1\n",
    "    if i > 50:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "\n",
    "for sent in sentence_list:\n",
    "    for word in word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('where \\n\\n\\n\\n\\nr\\n\\nt\\n\\n\\n\\n\\n{\\\\di play tyle r_{t}}\\n\\n i  the reward at  tep \\n\\n\\n\\nt\\n\\n\\n{\\\\di play tyle t}\\n\\n, \\n\\n\\n\\nγ\\n∈\\n[\\n0\\n,\\n1\\n)\\n\\n\\n{\\\\di play tyle \\\\gamma \\\\in [0,1)}\\n\\n i  the di count-rate.', 6.609756097560975)\n",
      "('Both algorithm  compute a  equence of function  \\n\\n\\n\\n\\nQ\\n\\nk\\n\\n\\n\\n\\n{\\\\di play tyle Q_{k}}\\n\\n (\\n\\n\\n\\nk\\n=\\n0\\n,\\n1\\n,\\n2\\n,\\n…\\n\\n\\n{\\\\di play tyle k=0,1,2,\\\\ldot  }\\n\\n) that converge to \\n\\n\\n\\n\\nQ\\n\\n∗\\n\\n\\n\\n\\n{\\\\di play tyle Q^{*}}\\n\\n.', 6.195121951219512)\n",
      "('Value function \\n\\n\\n\\n\\nV\\n\\nπ\\n\\n\\n(\\n \\n)\\n\\n\\n{\\\\di play tyle V_{\\\\pi }( )}\\n\\n i  defined a  the expected return  tarting with  tate \\n\\n\\n\\n \\n\\n\\n{\\\\di play tyle  }\\n\\n, i.e.', 5.060975609756098)\n",
      "('At each time t, the agent receive  the current  tate \\n\\n\\n\\n\\n \\n\\nt\\n\\n\\n\\n\\n{\\\\di play tyle  _{t}}\\n\\n and reward \\n\\n\\n\\n\\nr\\n\\nt\\n\\n\\n\\n\\n{\\\\di play tyle r_{t}}\\n\\n.', 4.609756097560976)\n",
      "('0\\n\\n\\n=\\n \\n\\n\\n{\\\\di play tyle  _{0}= }\\n\\n, and  ucce ively following policy \\n\\n\\n\\nπ\\n\\n\\n{\\\\di play tyle \\\\pi }\\n\\n.', 4.121951219512195)\n",
      "('Reinforcement learning require  clever exploration mechani m ; randomly  electing action , without reference to an e timated probability di tribution,  how  poor performance.', 3.3414634146341466)\n",
      "('Reinforcement learning i  one of three ba ic machine learning paradigm , along ide  upervi ed learning and un upervi ed learning.', 3.304878048780488)\n",
      "('Defining the performance function by\\nunder mild condition  thi  function will be differentiable a  a function of the parameter vector \\n\\n\\n\\nθ\\n\\n\\n{\\\\di play tyle \\\\theta }\\n\\n.', 3.158536585365854)\n",
      "('If the gradient of \\n\\n\\n\\nρ\\n\\n\\n{\\\\di play tyle \\\\rho }\\n\\n wa  known, one could u e gradient a cent.', 3.134146341463415)\n",
      "('Alternatively, with probability \\n\\n\\n\\nε\\n\\n\\n{\\\\di play tyle \\\\varep ilon }\\n\\n, exploration i  cho en, and the action i  cho en uniformly at random.', 2.804878048780488)\n",
      "('In economic  and game theory, reinforcement learning may be u ed to explain how equilibrium may ari e under bounded rationality.', 2.4268292682926833)\n",
      "('Linear function approximation  tart  with a mapping \\n\\n\\n\\nϕ\\n\\n\\n{\\\\di play tyle \\\\phi }\\n\\n that a ign  a finite-dimen ional vector to each  tate-action pair.', 2.426829268292683)\n",
      "('A determini tic  tationary policy determini tically  elect  action  ba ed on the current  tate.', 2.402439024390244)\n",
      "('The two main approache  for achieving thi  are value function e timation and direct policy  earch.', 2.378048780487805)\n",
      "('The work on learning ATARI game  by Google DeepMind increa ed attention to deep reinforcement learning or end-to-end reinforcement learning.', 2.329268292682927)\n",
      "('Value iteration can al o be u ed a  a  tarting point, giving ri e to the Q-learning algorithm and it  many variant .', 2.1585365853658542)\n",
      "('Mo t current algorithm  do thi , giving ri e to the cla  of generalized policy iteration algorithm .', 2.134146341463415)\n",
      "('Another i  that variance of the return  may be large, which require  many  ample  to accurately e timate the return of each policy.', 2.1341463414634143)\n",
      "('Hence, roughly  peaking, the value function e timate  \"how good\" it i  to be in a given  tate.', 2.0853658536585367)\n",
      "('Thi  approach extend  reinforcement learning by u ing a deep neural network and without explicitly de igning the  tate  pace.', 2.0365853658536586)\n",
      "('A uming full knowledge of the MDP, the two ba ic approache  to compute the optimal action-value function are value iteration and policy iteration.', 2.0121951219512195)\n",
      "('Many policy  earch method  may get  tuck in local optima (a  they are ba ed on local  earch).', 2.0121951219512195)\n",
      "('In inver e reinforcement learning (IRL), no reward function i  given.', 1.9390243902439026)\n",
      "('U ing the  o-called compatible function approximation method compromi e  generality and efficiency.', 1.9268292682926829)\n",
      "('Policy iteration con i t  of two  tep : policy evaluation and policy improvement.', 1.9024390243902438)\n",
      "('In a ociative reinforcement learning ta k , the learning  y tem interact  in a clo ed loop with it  environment.', 1.8902439024390247)\n",
      "('However, reinforcement learning convert  both planning problem  to machine learning problem .', 1.878048780487805)\n",
      "('The ca e of ( mall) finite Markov deci ion proce e  i  relatively well under tood.', 1.8170731707317076)\n",
      "('Monte Carlo method  can be u ed in an algorithm that mimic  policy iteration.', 1.8048780487804879)\n",
      "('Policy  earch method  have been u ed in the robotic  context.', 1.6951219512195124)\n",
      "('In both ca e , the  et of action  available to the agent can be re tricted.', 1.6829268292682926)\n",
      "('In order to addre  the fifth i ue, function approximation method  are u ed.', 1.5853658536585367)\n",
      "('Thu , reinforcement learning i  particularly well- uited to problem  that include a long-term ver u   hort-term reward trade-off.', 1.5365853658536586)\n",
      "('The e include  imulated annealing, cro -entropy  earch or method  of evolutionary computation.', 1.402439024390244)\n",
      "('The idea i  to mimic ob erved behavior, which i  often optimal or clo e to optimal.', 1.3902439024390245)\n",
      "('Policy  earch method  may converge  lowly given noi y data.', 1.3780487804878052)\n",
      "('Method  ba ed on temporal difference  al o overcome the fourth i ue.', 1.3170731707317076)\n",
      "('Method  ba ed on idea  from nonparametric  tati tic  (which can be  een to con truct their own feature ) have been explored.', 1.2926829268292686)\n",
      "('Monte Carlo i  u ed in the policy evaluation  tep.', 1.231707317073171)\n",
      "('In the operation  re earch and control literature, reinforcement learning i  called approximate dynamic programming, or neuro-dynamic programming.', 1.207317073170732)\n",
      "('Since an analytic expre ion for the gradient i  not available, only a noi y e timate i  available.', 1.207317073170732)\n",
      "('The brute force approach entail  two  tep :\\nOne problem with thi  i  that the number of policie  can be large, or even infinite.', 1.1463414634146343)\n",
      "('Thu , we di count it  effect).', 1.0853658536585367)\n",
      "('The algorithm mu t find a policy with maximum expected return.', 1.0853658536585367)\n",
      "('For example, thi  happen  in epi odic problem  when the trajectorie  are long and the variance of the return  i  large.', 1.0609756097560976)\n",
      "('The two approache  available are gradient-ba ed and gradient-free method .', 1.0121951219512195)\n",
      "('Again, an optimal policy can alway  be found among t  tationary policie .', 0.9146341463414633)\n",
      "('Thi  fini he  the de cription of the policy evaluation  tep.', 0.9024390243902439)\n",
      "('Many actor critic method  belong to thi  category.', 0.7926829268292683)\n",
      "('In tead, the reward function i  inferred given an ob erved behavior from an expert.', 0.7804878048780488)\n",
      "('In  ummary, the knowledge of the optimal action-value function alone  uffice  to know how to act optimally.', 0.7317073170731709)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "sorted_sentence_dict = {k: v for k, v in sorted(sentence_scores.items(), key=lambda item_: item_[1], reverse=True)}\n",
    "\n",
    "for item in sorted_sentence_dict.items():\n",
    "    print(item)\n",
    "    i += 1\n",
    "    if i > 50:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['where \\n\\n\\n\\n\\nr\\n\\nt\\n\\n\\n\\n\\n{\\\\di play tyle r_{t}}\\n\\n i  the reward at  tep \\n\\n\\n\\nt\\n\\n\\n{\\\\di play tyle t}\\n\\n, \\n\\n\\n\\nγ\\n∈\\n[\\n0\\n,\\n1\\n)\\n\\n\\n{\\\\di play tyle \\\\gamma \\\\in [0,1)}\\n\\n i  the di count-rate.', 'Both algorithm  compute a  equence of function  \\n\\n\\n\\n\\nQ\\n\\nk\\n\\n\\n\\n\\n{\\\\di play tyle Q_{k}}\\n\\n (\\n\\n\\n\\nk\\n=\\n0\\n,\\n1\\n,\\n2\\n,\\n…\\n\\n\\n{\\\\di play tyle k=0,1,2,\\\\ldot  }\\n\\n) that converge to \\n\\n\\n\\n\\nQ\\n\\n∗\\n\\n\\n\\n\\n{\\\\di play tyle Q^{*}}\\n\\n.', 'Value function \\n\\n\\n\\n\\nV\\n\\nπ\\n\\n\\n(\\n \\n)\\n\\n\\n{\\\\di play tyle V_{\\\\pi }( )}\\n\\n i  defined a  the expected return  tarting with  tate \\n\\n\\n\\n \\n\\n\\n{\\\\di play tyle  }\\n\\n, i.e.', 'At each time t, the agent receive  the current  tate \\n\\n\\n\\n\\n \\n\\nt\\n\\n\\n\\n\\n{\\\\di play tyle  _{t}}\\n\\n and reward \\n\\n\\n\\n\\nr\\n\\nt\\n\\n\\n\\n\\n{\\\\di play tyle r_{t}}\\n\\n.', '0\\n\\n\\n=\\n \\n\\n\\n{\\\\di play tyle  _{0}= }\\n\\n, and  ucce ively following policy \\n\\n\\n\\nπ\\n\\n\\n{\\\\di play tyle \\\\pi }\\n\\n.', 'Reinforcement learning require  clever exploration mechani m ; randomly  electing action , without reference to an e timated probability di tribution,  how  poor performance.', 'Reinforcement learning i  one of three ba ic machine learning paradigm , along ide  upervi ed learning and un upervi ed learning.']\n",
      "where \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r\n",
      "\n",
      "t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\di play tyle r_{t}}\n",
      "\n",
      " i  the reward at  tep \n",
      "\n",
      "\n",
      "\n",
      "t\n",
      "\n",
      "\n",
      "{\\di play tyle t}\n",
      "\n",
      ", \n",
      "\n",
      "\n",
      "\n",
      "γ\n",
      "∈\n",
      "[\n",
      "0\n",
      ",\n",
      "1\n",
      ")\n",
      "\n",
      "\n",
      "{\\di play tyle \\gamma \\in [0,1)}\n",
      "\n",
      " i  the di count-rate. Both algorithm  compute a  equence of function  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Q\n",
      "\n",
      "k\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\di play tyle Q_{k}}\n",
      "\n",
      " (\n",
      "\n",
      "\n",
      "\n",
      "k\n",
      "=\n",
      "0\n",
      ",\n",
      "1\n",
      ",\n",
      "2\n",
      ",\n",
      "…\n",
      "\n",
      "\n",
      "{\\di play tyle k=0,1,2,\\ldot  }\n",
      "\n",
      ") that converge to \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Q\n",
      "\n",
      "∗\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\di play tyle Q^{*}}\n",
      "\n",
      ". Value function \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "\n",
      "π\n",
      "\n",
      "\n",
      "(\n",
      " \n",
      ")\n",
      "\n",
      "\n",
      "{\\di play tyle V_{\\pi }( )}\n",
      "\n",
      " i  defined a  the expected return  tarting with  tate \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "{\\di play tyle  }\n",
      "\n",
      ", i.e. At each time t, the agent receive  the current  tate \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\di play tyle  _{t}}\n",
      "\n",
      " and reward \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "r\n",
      "\n",
      "t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{\\di play tyle r_{t}}\n",
      "\n",
      ". 0\n",
      "\n",
      "\n",
      "=\n",
      " \n",
      "\n",
      "\n",
      "{\\di play tyle  _{0}= }\n",
      "\n",
      ", and  ucce ively following policy \n",
      "\n",
      "\n",
      "\n",
      "π\n",
      "\n",
      "\n",
      "{\\di play tyle \\pi }\n",
      "\n",
      ". Reinforcement learning require  clever exploration mechani m ; randomly  electing action , without reference to an e timated probability di tribution,  how  poor performance. Reinforcement learning i  one of three ba ic machine learning paradigm , along ide  upervi ed learning and un upervi ed learning.\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "print(summary_sentences)\n",
    "print(summary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}