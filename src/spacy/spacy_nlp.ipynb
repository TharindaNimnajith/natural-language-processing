{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Tokenization - Segmenting text into words, punctuations marks etc.\n",
    "\n",
    "# Part-of-speech (POS) Tagging - Assigning word types to tokens, like verb or noun.\n",
    "\n",
    "# Dependency Parsing - Assigning syntactic dependency labels, describing the relations\n",
    "# between individual tokens, like subject or object.\n",
    "\n",
    "# Lemmatization\t- Assigning the base forms of words. For example, the lemma of 'was'\n",
    "# is 'be', and the lemma of 'rats' is 'rat'.\n",
    "\n",
    "# Sentence Boundary Detection (SBD)\t- Finding and segmenting individual sentences.\n",
    "\n",
    "# Named Entity Recognition (NER) - Labelling named 'real-world' objects, like persons,\n",
    "# companies or locations.\n",
    "\n",
    "# Entity Linking (EL) - Disambiguating textual entities to unique identifiers in a\n",
    "# knowledge base.\n",
    "\n",
    "# Similarity - Comparing words, text spans and documents and how similar they are to each\n",
    "# other.\n",
    "\n",
    "# Text Classification - Assigning categories or labels to a whole document, or parts of a\n",
    "# document.\n",
    "\n",
    "# Rule-based Matching - Finding sequences of tokens based on their texts and linguistic\n",
    "# annotations, similar to regular expressions.\n",
    "\n",
    "# Training - Updating and improving a statistical model’s predictions.\n",
    "\n",
    "# Serialization\t- Saving objects to files or byte strings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN advcl xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "# Text: The original word text.\n",
    "# Lemma: The base form of the word.\n",
    "# POS: The simple Universal POS part-of-speech tag.\n",
    "# Tag: The detailed part-of-speech tag.\n",
    "# Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "# Shape: The word shape – capitalization, punctuation, digits.\n",
    "# is alpha: Is the token an alpha character?\n",
    "# is stop: Is the token part of a stop list, i.e. the most common words of the language?\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_,\n",
    "          token.is_alpha, token.is_stop)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "# ent: named entity\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 7.0336733 False\n",
      "cat True 6.6808186 False\n",
      "banana True 6.700014 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "# Text: The original token text.\n",
    "# has vector: Does the token have a vector representation?\n",
    "# Vector norm: The L2 norm of the token’s vector (sqrt of the sum of the values squared)\n",
    "# OOV: Out-of-vocabulary\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "tokens = nlp('dog cat banana afskfsd')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.7799485853415737\n",
      "salty fries <-> hamburgers 0.7304624\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')  # make sure to use larger package!\n",
    "\n",
    "doc1 = nlp('I like salty fries and hamburgers.')\n",
    "doc2 = nlp('Fast food tastes very good.')\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, '<->', doc2, doc1.similarity(doc2))\n",
    "\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "print(french_fries, '<->', burgers, french_fries.similarity(burgers))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('I love coffee')\n",
    "\n",
    "print(doc.vocab.strings['coffee'])\n",
    "print(doc.vocab.strings[3197928453018144401])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "# Text: The original text of the lexeme.\n",
    "# Orth: The hash value of the lexeme.\n",
    "# Shape: The abstract word shape of the lexeme.\n",
    "# Prefix: By default, the first letter of the word string.\n",
    "# Suffix: By default, the last three letters of the word string.\n",
    "# is alpha: Does the lexeme consist of alphabetic characters?\n",
    "# is digit: Does the lexeme consist of digits?\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('I love coffee')\n",
    "\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'be', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode PERSON\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text = ('When Sebastian Thrun started working on self-driving cars at '\n",
    "        'Google in 2007, few people outside of the company took him '\n",
    "        'seriously. “I can tell you very senior CEOs of major American '\n",
    "        'car companies would shake my hand and turn away because I wasn’t '\n",
    "        'worth talking to,” said Thrun, in an interview with Recode earlier '\n",
    "        'this week.')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print('Noun phrases:', [chunk.text for chunk in doc.noun_chunks])\n",
    "print('Verbs:', [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angela Merkel\n",
      "Barack Obama\n",
      "Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "terms = ['Barack Obama', 'Angela Merkel', 'Washington, D.C.']\n",
    "\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add('TerminologyList', patterns)\n",
    "\n",
    "doc = nlp('German Chancellor Angela Merkel and US President Barack Obama '\n",
    "          'converse in the Oval Office inside the White House in Washington, D.C.')\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on lowercase token text: angela merkel\n",
      "Matched based on lowercase token text: barack Obama\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "patterns = [nlp.make_doc(name) for name in ['Angela Merkel', 'Barack Obama']]\n",
    "matcher.add('Names', patterns)\n",
    "\n",
    "doc = nlp('angela merkel and us president barack Obama')\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print('Matched based on lowercase token text:', doc[start:end])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched based on token shape: 192.168.1.1\n",
      "Matched based on token shape: 192.168.2.1\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='SHAPE')\n",
    "matcher.add('IP', [nlp('127.0.0.1'), nlp('127.127.0.0')])\n",
    "\n",
    "doc = nlp('Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.')\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print('Matched based on token shape:', doc[start:end])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}